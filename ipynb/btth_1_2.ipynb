{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "interpreter": {
   "hash": "25355d53bdc9b0feb20764a4fadf8fea7905d79df72ba215212bfe63b25c590d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 2, 0, 1, 0, 2, 1, 2, 1, 1, 0, 0, 1, 0, 2, 1, 2,\n",
       "       2, 1, 1, 0, 0, 2, 2, 0, 2, 2, 2, 2, 1, 0, 2, 2, 0, 1, 1, 1, 1, 2,\n",
       "       1])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree \n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(y_pred) == type(y_test), 'in correct type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function recall_score in module sklearn.metrics._classification:\n\nrecall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n    Compute the recall.\n    \n    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n    true positives and ``fn`` the number of false negatives. The recall is\n    intuitively the ability of the classifier to find all the positive samples.\n    \n    The best value is 1 and the worst value is 0.\n    \n    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n    \n    Parameters\n    ----------\n    y_true : 1d array-like, or label indicator array / sparse matrix\n        Ground truth (correct) target values.\n    \n    y_pred : 1d array-like, or label indicator array / sparse matrix\n        Estimated targets as returned by a classifier.\n    \n    labels : array-like, default=None\n        The set of labels to include when ``average != 'binary'``, and their\n        order if ``average is None``. Labels present in the data can be\n        excluded, for example to calculate a multiclass average ignoring a\n        majority negative class, while labels not present in the data will\n        result in 0 components in a macro average. For multilabel targets,\n        labels are column indices. By default, all labels in ``y_true`` and\n        ``y_pred`` are used in sorted order.\n    \n        .. versionchanged:: 0.17\n           Parameter `labels` improved for multiclass problem.\n    \n    pos_label : str or int, default=1\n        The class to report if ``average='binary'`` and the data is binary.\n        If the data are multiclass or multilabel, this will be ignored;\n        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n        scores for that label only.\n    \n    average : {'micro', 'macro', 'samples', 'weighted', 'binary'}             default='binary'\n        This parameter is required for multiclass/multilabel targets.\n        If ``None``, the scores for each class are returned. Otherwise, this\n        determines the type of averaging performed on the data:\n    \n        ``'binary'``:\n            Only report results for the class specified by ``pos_label``.\n            This is applicable only if targets (``y_{true,pred}``) are binary.\n        ``'micro'``:\n            Calculate metrics globally by counting the total true positives,\n            false negatives and false positives.\n        ``'macro'``:\n            Calculate metrics for each label, and find their unweighted\n            mean.  This does not take label imbalance into account.\n        ``'weighted'``:\n            Calculate metrics for each label, and find their average weighted\n            by support (the number of true instances for each label). This\n            alters 'macro' to account for label imbalance; it can result in an\n            F-score that is not between precision and recall.\n        ``'samples'``:\n            Calculate metrics for each instance, and find their average (only\n            meaningful for multilabel classification where this differs from\n            :func:`accuracy_score`).\n    \n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n    \n    zero_division : \"warn\", 0 or 1, default=\"warn\"\n        Sets the value to return when there is a zero division. If set to\n        \"warn\", this acts as 0, but warnings are also raised.\n    \n    Returns\n    -------\n    recall : float (if average is not None) or array of float of shape\n        (n_unique_labels,)\n        Recall of the positive class in binary classification or weighted\n        average of the recall of each class for the multiclass task.\n    \n    See Also\n    --------\n    precision_recall_fscore_support, balanced_accuracy_score,\n    multilabel_confusion_matrix\n    \n    Notes\n    -----\n    When ``true positive + false negative == 0``, recall returns 0 and raises\n    ``UndefinedMetricWarning``. This behavior can be modified with\n    ``zero_division``.\n    \n    Examples\n    --------\n    >>> from sklearn.metrics import recall_score\n    >>> y_true = [0, 1, 2, 0, 1, 2]\n    >>> y_pred = [0, 2, 1, 0, 0, 1]\n    >>> recall_score(y_true, y_pred, average='macro')\n    0.33...\n    >>> recall_score(y_true, y_pred, average='micro')\n    0.33...\n    >>> recall_score(y_true, y_pred, average='weighted')\n    0.33...\n    >>> recall_score(y_true, y_pred, average=None)\n    array([1., 0., 0.])\n    >>> y_true = [0, 0, 0, 0, 0, 0]\n    >>> recall_score(y_true, y_pred, average=None)\n    array([0.5, 0. , 0. ])\n    >>> recall_score(y_true, y_pred, average=None, zero_division=1)\n    array([0.5, 1. , 1. ])\n\n"
     ]
    }
   ],
   "source": [
    "help(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy 0.9333333333333333\nrecall 0.9333333333333333\nprecision 0.9333333333333333\n\n confusion matrix \n [[11  0  0]\n [ 0 17  2]\n [ 0  1 14]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score \n",
    "\n",
    "print('accuracy', accuracy_score(y_test, y_pred))\n",
    "print('recall', recall_score(y_test, y_pred, average='micro'))\n",
    "print('precision', precision_score(y_test, y_pred, average='micro'))\n",
    "print('\\n confusion matrix \\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}